# ğŸ’° AI-Powered Employee Salary Prediction System

A professional machine learning system with a web interface that predicts employee salaries using ensemble learning techniques. Built with real Kaggle data and featuring 6 state-of-the-art ML models.

![Python](https://img.shields.io/badge/Python-3.14+-blue)
![Status](https://img.shields.io/badge/Status-Production-green)
![ML](https://img.shields.io/badge/ML-Ensemble-orange)

## âœ¨ Features

- ğŸ¯ **Real-time Salary Predictions** - Interactive web interface
- ğŸ¤– **6 ML Models** - Random Forest, XGBoost, LightGBM, Stacking, etc.
- ğŸ“Š **Data Insights** - Interactive visualizations and analytics
- ğŸ“ˆ **Model Comparison** - Compare predictions across all models
- ğŸ’¼ **Real Data** - Trained on 607 real salary records from Kaggle
- ğŸ¨ **Professional UI** - Modern, responsive Streamlit interface

## ğŸš€ Quick Start

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Train Models (First Time Only)
```bash
python main.py
```

### 3. Launch Web Application
```bash
streamlit run app.py
```

Or double-click `run_app.bat` (Windows)

The web app will open at `http://localhost:8501`

## ğŸ¤– Machine Learning Models

### Ensemble Models Implemented
1. **ğŸŒ² Random Forest** - Bagging with decision trees
2. **ğŸ“ˆ Gradient Boosting** - Sequential boosting
3. **âš¡ XGBoost** - Extreme gradient boosting
4. **ğŸ’¡ LightGBM** - Light gradient boosting
5. **ğŸ—³ï¸ Voting Ensemble** - Average predictions
6. **ğŸ—ï¸ Stacking Ensemble** - Meta-learning (Best: RÂ² = 0.51)

## ğŸ“ Project Structure

```
Project PBEL/
â”‚
â”œâ”€â”€ config.py                  # Configuration and hyperparameters
â”œâ”€â”€ data_loader.py             # Data loading and preprocessing
â”œâ”€â”€ models.py                  # Machine learning models
â”œâ”€â”€ visualization.py           # Visualization utilities
â”œâ”€â”€ main.py                    # Main pipeline script
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ README.md                  # Project documentation
â”‚
â”œâ”€â”€ data/                      # Data directory
â”‚   â”œâ”€â”€ salary_data.csv        # Raw data
â”‚   â””â”€â”€ processed_data.csv     # Processed data
â”‚
â”œâ”€â”€ models/                    # Saved models
â”‚   â”œâ”€â”€ random_forest.joblib
â”‚   â”œâ”€â”€ gradient_boosting.joblib
â”‚   â”œâ”€â”€ xgboost.joblib
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ results/                   # Results and visualizations
    â”œâ”€â”€ validation_results.csv
    â”œâ”€â”€ test_results.csv
    â”œâ”€â”€ model_comparison.png
    â”œâ”€â”€ predictions_vs_actual.png
    â”œâ”€â”€ residuals.png
    â”œâ”€â”€ feature_importance.png
    â””â”€â”€ data_distribution.png
```

## ğŸš€ Getting Started

### Prerequisites

- Python 3.8 or higher
- pip package manager

### Installation

1. Install required packages:
```bash
pip install -r requirements.txt
```

### Usage

Run the complete pipeline:
```bash
python main.py
```

This will:
1. Load/create sample data
2. Preprocess and split data
3. Train all models (8 different models)
4. Evaluate models on validation and test sets
5. Generate visualizations
6. Save models and results

## ğŸ“ˆ Model Evaluation Metrics

The models are evaluated using:
- **RMSE** (Root Mean Squared Error) - Lower is better
- **MAE** (Mean Absolute Error) - Lower is better
- **RÂ² Score** (Coefficient of Determination) - Higher is better (max 1.0)
- **MAPE** (Mean Absolute Percentage Error) - Lower is better

## ğŸ¨ Visualizations

The project generates several visualizations:
1. **Data Distribution** - Salary distribution and feature analysis
2. **Model Comparison** - Performance comparison across all models
3. **Predictions vs Actual** - Scatter plot showing prediction accuracy
4. **Residuals Analysis** - Understanding prediction errors
5. **Feature Importance** - Most important features for prediction

## ğŸ”§ Configuration

Edit `config.py` to modify:
- Model hyperparameters
- Data paths
- Train/test split ratios
- Feature lists
- Random seed for reproducibility

## ğŸ“Š Expected Results

The ensemble models typically achieve:
- **RÂ² Score**: 0.85 - 0.95
- **RMSE**: $5,000 - $10,000
- **MAPE**: 5% - 10%

Best performing models are usually:
1. Stacking Ensemble
2. Voting Ensemble
3. XGBoost
4. LightGBM

## ğŸŒŸ Key Features

### Data Processing
- Automatic handling of missing values
- Label encoding for categorical features
- Standard scaling for numerical features
- Train/validation/test split

### Model Training
- Multiple ensemble techniques
- Cross-validation support
- Hyperparameter optimization ready
- Model persistence (save/load)

### Evaluation
- Comprehensive metrics
- Feature importance analysis
- Model comparison
- Residual analysis

## ğŸ“ Notes

### Using Kaggle Data

To use real data from Kaggle:

1. Install Kaggle API:
```bash
pip install kaggle
```

2. Set up Kaggle credentials:
   - Go to your Kaggle account settings
   - Create new API token
   - Place `kaggle.json` in `~/.kaggle/`

3. Download dataset:
```bash
kaggle datasets download -d <dataset-name>
```

4. Update `data_loader.py` to load your dataset

### Sample Datasets on Kaggle
- [Data Science Salaries 2023](https://www.kaggle.com/datasets/ruchi798/data-science-job-salaries)
- [Salary Prediction Dataset](https://www.kaggle.com/datasets/mohithsairamreddy/salary-data)
- [Tech Salaries](https://www.kaggle.com/datasets/thedevastator/jobs-dataset-from-glassdoor)

## ğŸ”„ Future Enhancements

- [ ] Hyperparameter tuning with GridSearchCV/RandomizedSearchCV
- [ ] Deep learning models (Neural Networks)
- [ ] Feature engineering and selection
- [ ] Cross-validation for ensemble models
- [ ] Real-time prediction API
- [ ] Interactive web dashboard
- [ ] Outlier detection and handling
- [ ] More sophisticated feature interactions

## ğŸ“š Dependencies

- pandas: Data manipulation
- numpy: Numerical computations
- scikit-learn: ML models and preprocessing
- xgboost: Gradient boosting
- lightgbm: Light gradient boosting
- matplotlib: Visualization
- seaborn: Statistical visualization
- joblib: Model persistence

## ğŸ¤ Contributing

Feel free to fork this project and submit pull requests with improvements!

## ğŸ“„ License

This project is open source and available under the MIT License.

## ğŸ‘¨â€ğŸ’» Author

Created for demonstrating ensemble machine learning techniques for salary prediction.

## ğŸ“§ Contact

For questions or suggestions, please open an issue in the repository.

---

**Happy Predicting! ğŸ¯**
# Salary-prediction
